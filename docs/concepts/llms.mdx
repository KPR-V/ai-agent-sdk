---
title: LLMs
description: Overview of supported language models, including OpenAI, Anthropic, and Google Gemini, Grok, and more.
icon: "computer-classic"
---

Each LLM is an adapter around a language model provider and the specific model version, eg: `gpt-4o-mini`. Each [Agent](./agents) can pick their own model and a [ZeeWorkflow](./zeeworkflows) can be configured to use a specific LLM as default.

```tsx
const llm = new LLM({
    provider: "OPEN_AI",
    name: "gpt-4o-mini",
    apiKey: process.env.OPENAI_API_KEY,
});

// Example with Ollama
const llm = new LLM({
    provider: "OLLAMA",
    name: "llama2", // or any model you've pulled in Ollama
    baseURL: "http://localhost:11434", // optional, defaults to this
});

// Example with Ollama with all options
const llm = new LLM({
    provider: "OLLAMA",
    name: "llama2",             // any model you've pulled in Ollama
    baseURL: "http://localhost:11434", // optional, defaults to this
    toolChoice: "auto",         // optional
});
```

## List of supported LLMs

### Open AI

<CodeGroup>

```plaintext OpenAI
"gpt-4"
"gpt-4-turbo"
"gpt-3.5-turbo"
"gpt-4o"
"gpt-4o-mini"
"o3-mini"
```

```plaintext DeepSeek
"deepseek-chat"
"deepseek-coder"
```

```plaintext Grok
"grok-2-latest"
"grok-beta"
```

```plaintext Gemini
"gemini-1.5-flash"
"gemini-1.5-pro"
```

```plaintext Ollama
Any model name that you have pulled in Ollama
Examples:
"llama2"
"codellama"
"mistral"
"neural-chat"
```

</CodeGroup>

## Environment Variables

<CodeGroup>

```plaintext OpenAI
OPENAI_API_KEY
```

```plaintext DeepSeek
DEEPSEEK_API_KEY
```

```plaintext Grok
GROK_API_KEY
```

```plaintext Gemini
GEMINI_API_KEY
```

```plaintext Ollama
OLLAMA_BASE_URL=http://localhost:11434  # Optional
```

</CodeGroup>

## Using Local Models with Ollama

To use Ollama with the SDK:

1. Install Ollama from [ollama.ai](https://ollama.ai)
2. Start the Ollama service:
```bash
ollama serve
```
3. Pull your desired model:
```bash
ollama pull llama2  # or any other model
```
4. Configure your agent to use Ollama:
```typescript
const agent = new Agent({
    name: "LocalAgent",
    model: {
        provider: "OLLAMA",
        name: "llama2",  // use the model name you pulled
        baseURL: "http://localhost:11434", // optional
    },
    description: "A locally-running AI assistant",
});
```

### Response Format
When using the SDK, you'll receive responses in the following structured format:

1. Complete Response Object:
```json
{
  "agent": "router",
  "messages": [
    {
      "role": "user",
      "content": "Your input message here"
    },
    {
      "role": "assistant",
      "content": {
        "thinking": "Detailed thought process...",
        "answer": "Final response..."
      }
    }
  ],
  "status": "finished",
  "children": []
}
```

2. Assistant Message Content Structure:
```json
{
  "role": "assistant",
  "content": {
    "thinking": "Internal reasoning process that shows how the model arrived at its answer...",
    "answer": "The final, concise response to the user's query..."
  }
}
```

The response includes:
- `thinking`: Detailed reasoning process (what the model is considering)
- `answer`: The final, refined response
- `status`: Completion status of the request
- `children`: Array for nested responses (if any)

You can access the latest response using:
```typescript
const lastMessage = result.messages[result.messages.length - 1]?.content;
console.log(lastMessage.content.thinking);  // Get thinking process
console.log(lastMessage.content.answer);    // Get final answer
```
