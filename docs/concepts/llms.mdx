---
title: LLMs
description: Overview of supported language models, including OpenAI, Anthropic, and Google Gemini, Grok, and more.
icon: "computer-classic"
---

Each LLM is an adapter around a language model provider and the specific model version, eg: `gpt-4o-mini`. Each [Agent](./agents) can pick their own model and a [ZeeWorkflow](./zeeworkflows) can be configured to use a specific LLM as default.

```tsx
const llm = new LLM({
    provider: "OPEN_AI",
    name: "gpt-4o-mini",
    apiKey: process.env.OPENAI_API_KEY,
});

// Example with Ollama
const llm = new LLM({
    provider: "OLLAMA",
    name: "llama2", // or any model you've pulled in Ollama
    baseURL: "http://localhost:11434", // optional, defaults to this
});
```

## List of supported LLMs

### Open AI

<CodeGroup>

```plaintext OpenAI
"gpt-4"
"gpt-4-turbo"
"gpt-3.5-turbo"
"gpt-4o"
"gpt-4o-mini"
"o3-mini"
```

```plaintext DeepSeek
"deepseek-chat"
"deepseek-coder"
```

```plaintext Grok
"grok-2-latest"
"grok-beta"
```

```plaintext Gemini
"gemini-1.5-flash"
"gemini-1.5-pro"
```

```plaintext Ollama
Any model name that you have pulled in Ollama
Examples:
"llama2"
"codellama"
"mistral"
"neural-chat"
```

</CodeGroup>

## Environment Variables

<CodeGroup>

```plaintext OpenAI
OPENAI_API_KEY
```

```plaintext DeepSeek
DEEPSEEK_API_KEY
```

```plaintext Grok
GROK_API_KEY
```

```plaintext Gemini
GEMINI_API_KEY
```

```plaintext Ollama
OLLAMA_BASE_URL=http://localhost:11434  # Optional
```

</CodeGroup>

## Using Local Models with Ollama

To use Ollama:

1. Install Ollama from [ollama.ai](https://ollama.ai)
2. Pull your desired model:
```bash
ollama pull llama2
```
3. Configure your agent to use Ollama:
```typescript
const agent = new Agent({
    name: "LocalAgent",
    model: {
        provider: "OLLAMA",
        name: "llama2",  // use the model name you pulled
    },
    description: "A locally-running AI assistant",
});
```
